---
title: Sun NFS
date: 2020-11-01
description: COMP 310
tags:
    - Operating Systems
    - Comp 310
---

[Paper Link: Design and Implementation or the SUN Network Filesystem](http://www.cs.yale.edu/homes/aspnes/pinewiki/attachments/NetworkFileSystems/nsf-design.pdf)

# Why is this paper interesting to you as a CS-related major? In other works, why did you choose this topic.

I'll admit that the main reason I chose this has my (second choice) topic: it is tangentially related to my research/passion project on The History of Computing over at the Software Systems Laboratory.

The history of Sun Microsystems, and thus their innovations like the Sun Network Filesystem, are pretty unique in terms of their success meant for the broad field of Computer Science. Sun was different from most of their competition because they were huge advocates of the open-source movement. NFS was one of their largest contributions to the movement, yet they were one of the most financially successful tech companies before Oracle bought them. I think Red Hat is the only other company firmly within the open-source movement that has experienced a large amount of monetary success in terms of comparable companies. Thus, this paper fits my hipsteresque ideals of the open-source movement and my rose-colored nostalgia for the early days of Computer Science, where money wasn't the primary motivator for everything. ‘


Since I can't wax poetic for a whole paper, on a more technical level, I worked for the I.T. Service Desk for Loyola before this semester. A lot of Faculty use the Windows Network Drive system or Apple's Network Drive system. I believe, although I'm certainly no expert, that NFS at least inspired both of these. It would be cool to learn the inner workings of something I used to spend hours troubleshooting for.

# What research or technical problem(s) does this paper aim to solve?
In short, NFS is meant to be cross-platform (meaning it would work the same regardless of the operating system and machine architectures) remote filesystem that still maintains the same speed of local file access. This primary goal is broken into five different components.
  Firstly, the protocols of the network file system should be independent of Unix's protocols. Secondly, files should be easily recoverable in case a server(s) crash. Thirdly, the network file system should provide access the same way as a local file system; Sandberg, Goldberg, Kleiman, Walsh, and Lyon call this "transparency access." Fourthly, due to the need for transparency access, the local Unix filesystem's semantics must be maintained for the network file system, I.E., the Filesystem Hierarchy Standard (FHS). Finally, the network file system should be 80% as fast as the local filesystem/disk.


# How does the proposed approach result in a solution to the problem? Describe each in at least one paragraph.
Sandberg, Goldberg, Kleiman, Walsh, and Lyon's approach consists of three core components of the overall file system.
 Firstly, the Network File System protocol, which is a set of procedures. The protocol uses the Sun Remote Procedure Call (RPC). The Sun Remote Procedure Call package (RPC) is transport-independent, meaning it can use UDP and I.P. protocols for its transport level. This solves the first problem that NFS's protocols should be independent of Unix's protocols.

Each procedure is broken down into its arguments, results, and effects on the file system. An outline of the procedures is given in the paper, such as remove(), mkdir(), etc. Each call is synchronous, meaning that a procedure has to finish before the user can do another procedure.

The protocols are stateless, meaning that each procedure contains all the necessary info to complete a server call on its own (rather than having state maintained on a server). This solves the second listed problem that files should be easily recoverable in case of a server crash. If a server crashes, the client simply resends request calls until it reconnects to the server, and no actual crash recovery is made.

  Secondly, the authors focus on the server implementation. As previously mentioned, the protocols are stateless, which affects how data is modified on UNIX/UNIX servers. This means that any modified data needs to be 'committed' to the local storage before the request is finished. Their implementation is as follows; any request/call that modifies data will flush the modified changes the cache to the disk drive before the call is returned.

  Thirdly, and most importantly, the focus switches to the client-side (I.E., the user interface) to NFS. The authors decided to allow the client to attach a remote filesystem using the mount program. This means that the client only has to deal with the hostname once, during the initial mounting, but it also means that remote files are only available after the client mounts the file address. In English, this means that users will have to mount the remote filesystem (AKA a network drive) once, rather than accessing the remote file using the syntax' host:path:[FILE PATH]'. All in all, this accomplishes the outlined goal of "transparency access."

Once mounted, procedures that affect the overall network file system are handled by the Virtual Filesystem interface (VFS). In contrast, procedures that affect singular files are managed by the Virtual Node (vnode) interface. These procedures are semantically the same as your typical UNIX commands; for example, 'mount' is a VFS procedure, and 'mkdir' is a vnode procedure. This solves the fourth problem, maintaining the semantics of the UNIX filesystem.

Here’s a very handy diagram of the overall interface:
![](KQRrPQDO6V9fkoa8kA5o9-_PVNHZXbRc7ViRiTZdk52Zs4PojUEj_LFltS35vFWVM7li-bVBrXC_DgS4c4B-GjxmJx8iOCNaFJ3CnF-KYxGXDA7skUfXy3V0YIR2JoCR8LYkHb6K.png)



The parameters for procedures are done by breaking the overall directory (as defined by FHS) into different nodes, rather than passing the whole file system path. This allows the filesystem to avoid tracking all the clients' mount points, which would render a stateless protocol, as mentioned previously, impossible. A cache of nodes is stored instead, for efficiency's sake. This accomplishes the 5th problem that accessing files on the NFS should be at least 80% as fast as accessing local files.

# What ideas in modern operating systems (which could include distributed systems, clouds, IoT, Fog, edge, etc.) are influenced by this paper?
Well, remote network drives, as I mentioned at the start of the paper, are massively influenced by NFS. From my basic research, NFS is still in use across different operating systems, which makes sense considering the goal of working across cross-platforms.

 Beyond the direct influences of NFS, I would argue that creating an easily distributable file system has at least indirectly lead to the creation of new emerging technologies. Having easy access to a network file system means it's easier to work remotely. Specifically, when I worked at the ITS service desk when COVID hit last year, staff didn't have to use a remote desktop to connect to their office computer; instead, I could help them connect by mounting a network drive. I also think it makes virtualization more viable in some use cases; for example, in COMP: 264 (Computer Systems), all the students had access to a portion of "Shannon's" directory.

I'm sure I'm missing many influences, but I don't know that much about operating systems, especially when it comes to stuff like protocols and their underlying design principles.
Are there any take-aways from this paper that could help you to be a better computer scientist/software engineer?



# Are there any take-aways from this paper that could help you to be a better computer scientist/software engineer.


I think this shows how vital learning some of the inner workings of computers are. After reading this, my confidence in my understanding of file systems drastically reduced. Sure, I knew some of the basic UNIX commands, but I had no idea what a remote procedure call was. The sad thing is I probably know more UNIX stuff than like 80% of computer-literate people.

With the advancement of modern-day languages/frameworks like Node.JS, Python, and specifically React.JS for me, it's so easy to be a lazy programmer. Building a cool site in React requires very little understanding of the world wide web's underlying principles, like TCP/IP protocol, HTTP, DNS, and much more. Similarly, creating a useful Python project requires little if no understanding of things like memory allocation or multithreading.

On the one hand, this is a good thing because it breaks down barriers to software development; it's a lot easier for someone without formal education to create something useful. On the other hand, it means that a smaller and smaller amount of people will actually understand the intricacies of the technologies that make frameworks like React even possible.

Ultimately I think reading these types of papers shows that classes like 264 and this one should be continued to be taught, even though it's frustrating coding in C when I just want to create a bunch of React Components without thinking about my actual computer. Personally, I try to use my terminal as much as possible even though I don't really need to to keep my wits as sharp as possible. I have a bash_profile set up with a bunch of commands I've made, and one of my small accomplishments in life is that I have one of the best (and only) AppleScript repositories on Github. All in all, we should try to balance our understanding of the fundamental principles of computer science with new and improved technologies.
